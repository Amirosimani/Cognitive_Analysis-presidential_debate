{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Amiros/anaconda/lib/python3.5/site-packages/IPython/core/magics/extension.py:47: UserWarning: %install_ext` is deprecated, please distribute your extension as a python package.\n",
      "  \"as a python package.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed autotime.py. To use it, type:\n",
      "  %load_ext autotime\n"
     ]
    }
   ],
   "source": [
    "%install_ext https://raw.github.com/cpcloud/ipython-autotime/master/autotime.py\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 207 ms\n"
     ]
    }
   ],
   "source": [
    "#setting up the file\n",
    "videoFile = 'sample.mp4'\n",
    "imagesFolder = \"/debate_frames\"\n",
    "vidcap = cv2.VideoCapture(videoFile)\n",
    "success,image = vidcap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting up parameters \n",
    "seconds = 5\n",
    "fps = vidcap.get(cv2.CAP_PROP_FPS) # Gets the frames per second\n",
    "multiplier = fps * seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "#extarcting every frame\n",
    "count = 0;\n",
    "while success:\n",
    "  success,image = vidcap.read()\n",
    "  cv2.imwrite(\"debate_frames/frame%d.jpg\" % count, image)     # save frame as JPEG file\n",
    "  if cv2.waitKey(10) == 27:                     # exit if Escape is hit\n",
    "      break\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#every Nth second\n",
    "while success:\n",
    "    frameId = int(round(vidcap.get(1)))\n",
    "    success, image = vidcap.read()\n",
    "\n",
    "    if frameId % multiplier == 0:\n",
    "        cv2.imwrite(\"debate_frames/frame%d.jpg\" % frameId, image)\n",
    "        \n",
    "    if cv2.waitKey(10) == 27:  # exit if Escape is hit\n",
    "        break\n",
    "\n",
    "vidcap.release()\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frameRate = vidcap.get(5) #frame rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving on S3 maybe?\n",
    "\n",
    "#### https://github.com/Microsoft/Cognitive-Face-Python/blob/master/cognitive_face/face.py ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Detecting gender in the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cognitive_face as CF\n",
    "from cognitive_face import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CF.Key.set('2ed7c5847ac640f6b0a404c72d0b96f2') #set the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect(image, face_id=True, landmarks=False, faceRectangle= False, attributes='gender'):\n",
    "    \n",
    "    url = 'detect'\n",
    "    headers, data, json = util.parse_image(image)\n",
    "    params = {\n",
    "        'returnFaceId': face_id and 'true' or 'false',\n",
    "        'returnFaceLandmarks': landmarks and 'true' or 'false',\n",
    "        'returnFaceAttributes': attributes,\n",
    "    }\n",
    "\n",
    "    return util.request('POST', url, headers=headers, params=params, json=json,\n",
    "                        data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = '/Users/Amiros/GitHub/debate/images/frame10.jpg' #puting the image location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detect(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image2 = '/Users/Amiros/GitHub/debate/images/frame11.jpg' #puting the image location\n",
    "detect(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the idea is to go through the whole fodler,\n",
    "# put everything in a dataframe,\n",
    "# and group them by gender\n",
    "\n",
    "## or I can use an album of H&T photos and then assign them to the respective ones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group(face_ids):\n",
    "    \"\"\"Divide candidate faces into groups based on face similarity.\n",
    "    Args:\n",
    "        face_ids: An array of candidate `face_id`s created by `face.detect`.\n",
    "            The maximum is 1000 faces.\n",
    "    Returns:\n",
    "        one or more groups of similar faces (ranked by group size) and a\n",
    "        messyGroup.\n",
    "    \"\"\"\n",
    "    url = 'group'\n",
    "    json = {\n",
    "        'faceIds': face_ids,\n",
    "    }\n",
    "\n",
    "    return util.request('POST', url, json=json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idLists = ('0f3399c9-7416-4203-b101-c42b5c1fbe3f', '25b2310b-ba4c-4d9f-a177-39ec1fb7fcf7', 'a1d34384-b6c2-404d-90eb-840815d61d64', '29ba22bf-2284-4cb0-a0a6-771ce0a47f1a') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group1 = group(idLists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group1['groups']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Emotion of groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib, base64, http.client\n",
    "\n",
    "body = '/Users/Amiros/GitHub/debate/images/frame11.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "_url = 'https://api.projectoxford.ai/emotion/v1.0/recognize'\n",
    "_key = '183aea69820e4dfda0176db816fc1f72' #Here you have to paste your primary key\n",
    "_maxNumRetries = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time \n",
    "import requests\n",
    "import cv2\n",
    "import operator\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import library to display results\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# Display images within Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processRequest( json, data, headers, params ):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to process the request to Project Oxford\n",
    "\n",
    "    Parameters:\n",
    "    json: Used when processing images from its URL. See API Documentation\n",
    "    data: Used when processing image read from disk. See API Documentation\n",
    "    headers: Used to pass the key information and the data type request\n",
    "    \"\"\"\n",
    "\n",
    "    retries = 0\n",
    "    result = None\n",
    "\n",
    "    while True:\n",
    "\n",
    "        response = requests.request( 'post', _url, json = json, data = data, headers = headers, params = params )\n",
    "\n",
    "        if response.status_code == 429: \n",
    "\n",
    "            print( \"Message: %s\" % ( response.json()['error']['message'] ) )\n",
    "\n",
    "            if retries <= _maxNumRetries: \n",
    "                time.sleep(1) \n",
    "                retries += 1\n",
    "                continue\n",
    "            else: \n",
    "                print( 'Error: failed after retrying!' )\n",
    "                break\n",
    "\n",
    "        elif response.status_code == 200 or response.status_code == 201:\n",
    "\n",
    "            if 'content-length' in response.headers and int(response.headers['content-length']) == 0: \n",
    "                result = None \n",
    "            elif 'content-type' in response.headers and isinstance(response.headers['content-type'], str): \n",
    "                if 'application/json' in response.headers['content-type'].lower(): \n",
    "                    result = response.json() if response.content else None \n",
    "                elif 'image' in response.headers['content-type'].lower(): \n",
    "                    result = response.content\n",
    "        else:\n",
    "            print( \"Error code: %d\" % ( response.status_code ) )\n",
    "            print( \"Message: %s\" % ( response.json()['error']['message'] ) )\n",
    "\n",
    "        break\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urlImage2 = 'https://raw.githubusercontent.com/Microsoft/ProjectOxford-ClientSDK/master/Face/Windows/Data/detection3.jpg'\n",
    "urlImage = 'https://s3-us-west-2.amazonaws.com/debateinemotion/T/t1.png'\n",
    "\n",
    "\n",
    "\n",
    "headers = dict()\n",
    "headers['Ocp-Apim-Subscription-Key'] = _key\n",
    "headers['Content-Type'] = 'application/json' \n",
    "\n",
    "json = { 'url': urlImage } \n",
    "data = None\n",
    "params = None\n",
    "\n",
    "result = processRequest( json, data, headers, params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 visualizing the image on jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def renderResultOnImage( result, img ):\n",
    "    \n",
    "    \"\"\"Display the obtained results onto the input image\"\"\"\n",
    "    \n",
    "    for currFace in result:\n",
    "        faceRectangle = currFace['faceRectangle']\n",
    "        cv2.rectangle( img,(faceRectangle['left'],faceRectangle['top']),\n",
    "                           (faceRectangle['left']+faceRectangle['width'], faceRectangle['top'] + faceRectangle['height']),\n",
    "                       color = (255,0,0), thickness = 5 )\n",
    "\n",
    "\n",
    "    for currFace in result:\n",
    "        faceRectangle = currFace['faceRectangle']\n",
    "        currEmotion = max(currFace['scores'].items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "\n",
    "        textToWrite = \"%s\" % ( currEmotion )\n",
    "        cv2.putText( img, textToWrite, (faceRectangle['left'],faceRectangle['top']-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if result is not None:\n",
    "    # Load the original image, fetched from the URL\n",
    "    arr = np.asarray( bytearray( requests.get( urlImage ).content ), dtype=np.uint8 )\n",
    "    img = cv2.cvtColor( cv2.imdecode( arr, -1 ), cv2.COLOR_BGR2RGB )\n",
    "\n",
    "    renderResultOnImage( result, img )\n",
    "\n",
    "    ig, ax = plt.subplots(figsize=(15, 20))\n",
    "    ax.imshow( img )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
